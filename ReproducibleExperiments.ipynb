{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reproducible Experiments\n",
    "In this notebook, the aim is to show you how to build a reproducible experiment within Azure. In this example, we're going to train a MNIST classifier using Tensorflow and show you how you can follow some practices to make this work reproducible. Later you can use this framework and apply it to other ML problems.\n",
    "\n",
    "The reason we chose MNIST is to pick a very simple example as the main focus is to build a reproducible experiment and not to learn a new algorithm or to build a complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:**:\n",
    "\n",
    "In order to practice all parts of the following Notebook, you first need to get a free Azure credit. If you don't have it, you can simply obtain it through this link: https://azure.microsoft.com/en-us/free/\n",
    "\n",
    "You can run this notebook on your local latop, Azure Notebooks (notebooks.azure.com) or Notebook VMs:\n",
    "- Local Laptop - the following packages has to be installed:\n",
    "    - Azureml-SDK - with notebook,widget extensions\n",
    "    - tensorflow==1.13\n",
    "- Azure Notebooks:\n",
    "    - This is a free notebook, all of the packages for an ML experiment is installed\n",
    "- AzureML Notebook:\n",
    "    - This is a premium notebook that you can choose the VM type. Avoid using this feature for the workshop as you may burn your credit before the end or the workshop.\n",
    "\n",
    "Once you chose the execution environment, you need to create an Azure Machine Learning Service. Follow this instruction to build one:\n",
    "\n",
    "The following text is copied from: https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup#create-a-workspace\n",
    "\n",
    "\n",
    "An Azure Machine Learning workspace is a foundational resource in the cloud that you use to experiment, train, and deploy machine learning models. It ties your Azure subscription and resource group to an easily consumed object in the service.\n",
    "\n",
    "You create a workspace via the Azure portal, a web-based console for managing your Azure resources.\n",
    "\n",
    "1. Sign in to the Azure portal by using the credentials for the Azure subscription you use.\n",
    "1. In the upper-left corner of Azure portal, select + Create a resource.\n",
    "1. Create a new resource\n",
    "1. Use the search bar to find Machine Learning service workspace.\n",
    "1. Select Machine Learning service workspace.\n",
    "1. In the Machine Learning service workspace pane, select Create to begin.\n",
    "1. Provide the following information to configure your new workspace:\n",
    "    - **Field\tDescription**\n",
    "    - **Workspace name**: type in **FirstExample**.\n",
    "    - **Subscription**: Select the Azure subscription that you want to use. (Your free credit)\n",
    "    - **Resource group**: type in **MLOpsWorkshop**\n",
    "    - **Location**: type in **westus2**\n",
    "1. After you are finished configuring the workspace, select Create.\n",
    "When the process is finished, a deployment success message appears.\n",
    "1. To view the new workspace, select Go to resource.\n",
    "\n",
    "\n",
    "You can explore the resource from two view:\n",
    "1. https://portal.azure.com (you can access all resources including Azure ML)\n",
    "1. https://ml.azure.com (recently released - still in preview and dedicated to Azure ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download the MNIST sample files from Yann Lecun website to our development environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/mnist/test-labels.gz', <http.client.HTTPMessage at 0x1c6ee072448>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.widgets import RunDetails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the AzureML SDK package to be able to communicate with Azure ML Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.83\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your subscription ID will be different replace the stirng with yours\n",
    "subscription_id = \"3df1840f-dd4b-4f54-a831-e20536439b3a\"\n",
    "resource_group = \"AzureML\"\n",
    "workspace_name = \"MTDemoWUS2\"\n",
    "workspace_region = \"westus2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an object from Workspace class. the Workspace object will point to the created Workspace we created through the portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Workspace class and check the azureml SDK version\n",
    "# exist_ok checks if workspace exists or not.\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MTDemoWUS2\n",
      "Azure region: westus2\n",
      "Subscription id: 3df1840f-dd4b-4f54-a831-e20536439b3a\n",
      "Resource group: AzureML\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a new Experiment\n",
    "\n",
    "In software engineering world we have a new feature to develop. In ML/Data Science world, we work on experiments.\n",
    "\n",
    "**Experiments** represent the collection of trials used to validate a user's hypothesis. We call each trial a run.\n",
    "\n",
    "Here we create a new experiment, we want to make sure everything related to this experiment is saved within the workspace and the lineage between each artifact is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='at-manulife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Track and Log Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two ways to start a trail (Run). Interactively and through batch submite. The start_loggin() method is the interactive way of starting a trail. It returns a Run object that we can use to log important metrics or the trail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key metrics can a single value for the accuracy of an ML model, a list of values representing the distribution or the data or an image showing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing log tracking action by creating a Run object in the Experiment\n",
    "run =  exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see that the **azureml.git.repository_uri** is poinint to the remote repo and the **azureml.git.branch** property is poining to the active branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'd7d73f7d-a107-4ff7-9b35-2b265aa4e592',\n",
       " 'target': 'sdk',\n",
       " 'status': 'Running',\n",
       " 'startTimeUtc': '2020-02-11T00:52:57.039037Z',\n",
       " 'properties': {'azureml.git.repository_uri': 'https://github.com/classicboyir/exp-repro.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/classicboyir/exp-repro.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '8431dc9407a3b7ce40ed7e7321568d825a2d02fe',\n",
       "  'mlflow.source.git.commit': '8431dc9407a3b7ce40ed7e7321568d825a2d02fe',\n",
       "  'azureml.git.dirty': 'True',\n",
       "  'ContentSnapshotId': '31ec425e-68a3-4e6f-9ac8-d16053d8fb22'},\n",
       " 'inputDatasets': [],\n",
       " 'logFiles': {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "# run = Run(experiment=exp, run_id='<get it from the result above>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunDetails** class helps you visualize the active run object. It creates a network connection with Azure ML Worspace to collect everything happening during the run. It gets updated every 15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30bea3d65204d9793e94dd8b92d65e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/at-manulife/runs/d7d73f7d-a107-4ff7-9b35-2b265aa4e592?wsid=/subscriptions/3df1840f-dd4b-4f54-a831-e20536439b3a/resourcegroups/AzureML/workspaces/MTDemoWUS2\", \"run_id\": \"d7d73f7d-a107-4ff7-9b35-2b265aa4e592\", \"run_properties\": {\"run_id\": \"d7d73f7d-a107-4ff7-9b35-2b265aa4e592\", \"created_utc\": \"2020-02-11T00:52:56.601313Z\", \"properties\": {\"azureml.git.repository_uri\": \"https://github.com/classicboyir/exp-repro.git\", \"mlflow.source.git.repoURL\": \"https://github.com/classicboyir/exp-repro.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"8431dc9407a3b7ce40ed7e7321568d825a2d02fe\", \"mlflow.source.git.commit\": \"8431dc9407a3b7ce40ed7e7321568d825a2d02fe\", \"azureml.git.dirty\": \"True\", \"ContentSnapshotId\": \"31ec425e-68a3-4e6f-9ac8-d16053d8fb22\"}, \"tags\": {}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {}, \"log_groups\": [], \"run_duration\": \"0:00:50\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"Your job is submitted in Azure cloud and we are monitoring to get logs...\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logs** function will log a single-valued or multi-valued metric under the current RUN. There are several types of logs, metric, table, row, image, etc.\n",
    "\n",
    "Every time you add a new metric, check the widget above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some metrics about the input dataset:\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "# Unzipping the input dataset and conver the data points into Numpy arrays\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record how the input dataset looks like\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "    \n",
    "run.log_image(name='{}-samples-of-input-dataset'.format(sample_size), plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dist = pd.DataFrame(data=y_train, columns=['test_values'])['test_values'].value_counts()\n",
    "\n",
    "run.log_table('digit_dist', {\"count\":list(dist.values), \"digits\":list(dist.index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('train_dataset_size', X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('test_dataset_size', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the collected metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_1:  {'metric_1': [1.1, 2, 2, 4, 4, 2.5, 6.3]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric_1': [1.1, 2, 2, 4, 4, 2.5, 6.3],\n",
       " '30-samples-of-input-dataset': 'aml://artifactId/ExperimentRun/dcid.d7d73f7d-a107-4ff7-9b35-2b265aa4e592/30-samples-of-input-dataset_1581382418.png',\n",
       " 'digit_dist': {'count': [6742,\n",
       "   6265,\n",
       "   6131,\n",
       "   5958,\n",
       "   5949,\n",
       "   5923,\n",
       "   5918,\n",
       "   5851,\n",
       "   5842,\n",
       "   5421],\n",
       "  'digits': [1, 7, 3, 2, 9, 0, 6, 8, 4, 5]},\n",
       " 'train_dataset_size': 60000,\n",
       " 'test_dataset_size': 10000}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('metric_1: ', run.get_metrics('metric_1'))\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Datasets\n",
    "\n",
    "As an important part of experiment reproducibility, you'd like to separate your dataset from the training code and the development environment. There are two major ways that you can achieve this. First you can use datastores to define the connection to an Azure data store such as Blob, SQL or Databricks table, then you leverage Datasets to access the actual files and version the reference to those file assets.\n",
    "\n",
    "### 2.1 Generate data references for the ML job\n",
    "\n",
    "Once an ML Workspace is created, a storage account is created with a default container (a logical container that works as a folder - this is refered to as bucket in AWS S3). The container is attached to the Workspace automatically as the default storage account. You can find it as **workspaceblobstore** under Datastores (https://ml.azure.com). This storage account can be used for test and development but should not be used in production scenarios. Because if you decide to delete the Workspace, the default storage account is also deleted which results in losing your data. So it's wiser to create a separate storage account and attach it to the ML Workspace.\n",
    "\n",
    "Here is how to access the default storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'workspaceblobstore'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_default_datastore returns the default datastore attached to the Workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name of the default datastore suggestion, it's a reference to a Blob storage. Blob storage is a general purpose data lake that can be used to store any type of binary, from image, to csv file. Here are other types of Azure Storage types that you can attach to the WorkSpace:\n",
    "\n",
    "- Azure File Share\n",
    "- Azure Data Lake\n",
    "- Azure Data Lake Gen2\n",
    "- Azure SQL Database\n",
    "- Azure PostgreSQL\n",
    "- Databricks File System\n",
    "\n",
    "The following is an example of registering (attaching) a new Blob storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.datastore import Datastore\n",
    "# blob_datastore = Datastore.register_azure_blob_container(\n",
    "#            workspace=ws,\n",
    "#            datastore_name='<datastore_name>',\n",
    "#            account_name='<account_name>', # Storage account name\n",
    "#            container_name='<container_name>', # Name of Azure blob container\n",
    "#            account_key='<account_key>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the files that I downloaded to this storage for longer retention. This datastore can be referenced later in my training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 4 files\n",
      "Uploading ./data/mnist\\test-images.gz\n",
      "Uploading ./data/mnist\\test-labels.gz\n",
      "Uploading ./data/mnist\\train-images.gz\n",
      "Uploading ./data/mnist\\train-labels.gz\n",
      "Uploaded ./data/mnist\\test-labels.gz, 1 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\train-labels.gz, 2 files out of an estimated total of 4\n"
     ]
    }
   ],
   "source": [
    "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate Datasets\n",
    "\n",
    "There are two different types of Datasets, TabularDataset and FileDataset. The Tabular can be used to access tabular like datasources, such as csv, SQL, Databricks, etc which FileDatasets can be used for binary datasets such as image, audio, etc. The following is a way to define a tabular dataset from an online source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# create a TabularDataset from a delimited file behind a public web url\n",
    "web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path)\n",
    "\n",
    "# To convert the Dataset into Spark (you need to have Spark installed on your development environment)\n",
    "# titanic_ds.take(3).to_spark_dataframe()\n",
    "\n",
    "# preview the first 3 rows of titanic_ds\n",
    "titanic_ds.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are references to a datasource (registered under datasources or available over internet). In other words, they don't keep the data, they are the reference definitiosn. You can register the defined Datasets under the Datasets section (accessible through ml.azure.com). Each time you register the dataset under the same name, you'll get a new version generated. Later you can access a particular version for the registered Dataset. As the dataset doesn't store your data, if you remove or change the data, the Dataset object doesn't help you roll back the change. Therefore, it's recommended to keep the data untouched. \n",
    "\n",
    "In order to keep the actual dataset, you can copy the data (using Azure Data Factory SDK or AzCopy CLI tool or [Azure SDK](https://github.com/Azure/azure-sdk-for-python/tree/master/sdk/storage/azure-storage-blob)) to clone the data to a new blob location and keep the under a new Dataset object.\n",
    "\n",
    "In the following cells, I've generated two versions of the Dataset. Every version of the titanic_ds can have different reference structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time you can retrive a particular version of Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'titanic_ds', \n",
    "                                 version = 1)\n",
    "\n",
    "titanic_ds_v1.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample code to access Parquet files: link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets\n",
    "# \n",
    "# # create a TabularDataset with time series trait\n",
    "# datastore_paths = [(datastore, 'weather/*/*/*/data.parquet')]\n",
    "# \n",
    "# # get a coarse timestamp column from the path pattern\n",
    "# dataset = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format='weather/{coarse_time:yyy/MM/dd}/data.parquet')\n",
    "# \n",
    "# # set coarse timestamp to the virtual column created, and fine grain timestamp from a column in the data\n",
    "# dataset = dataset.with_timestamp_columns(fine_grain_timestamp='datetime', coarse_grain_timestamp='coarse_time')\n",
    "# \n",
    "# # filter with time-series-trait-specific methods\n",
    "# data_slice = dataset.time_before(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_after(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_between(datetime(2019, 1, 1), datetime(2019, 2, 1))\n",
    "# data_slice = dataset.time_recent(timedelta(weeks=1, days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going back to our MNIST problem, let's create a Dataset object from the file uploaded to the default Datastore and register the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(ds, 'mnist/test-images.gz'),\n",
    " (ds, 'mnist/test-labels.gz'),\n",
    " (ds, 'mnist/train-images.gz'),\n",
    " (ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "mnist_dataset = Dataset.File.from_files(datastore_paths)\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's register it within the Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define a new Dataset under **mnist_dataset** by accessing the actual file over the internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_paths = [\n",
    "            'http://lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "\n",
    "mnist_dataset_web = Dataset.File.from_files(path = web_paths)\n",
    "\n",
    "mnist_dataset_web.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'mnist_dataset', \n",
    "                                 version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of registered files back\n",
    "mnist_dataset_v1.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the level of flexibility, I create a different dataset for each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_path_test_images = [(ds, 'mnist/test-images.gz')]\n",
    "datastore_path_test_labels = [(ds, 'mnist/test-labels.gz')]\n",
    "datastore_path_train_images = [(ds, 'mnist/train-images.gz')]\n",
    "datastore_path_train_labels = [(ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "dataset_test_images = Dataset.File.from_files(datastore_path_test_images)\n",
    "dataset_test_images.register(workspace=ws, name='mnist_test_images', description='MNIST input dataset for test images', create_new_version = True)\n",
    "\n",
    "dataset_test_labels = Dataset.File.from_files(datastore_path_test_labels)\n",
    "dataset_test_labels.register(workspace=ws, name='mnist_test_labels', description='MNIST input dataset for test labels', create_new_version = True)\n",
    "\n",
    "dataset_train_images = Dataset.File.from_files(datastore_path_train_images)\n",
    "dataset_train_images.register(workspace=ws, name='mnist_train_images', description='MNIST input dataset for train images', create_new_version = True)\n",
    "\n",
    "dataset_train_labels = Dataset.File.from_files(datastore_path_train_labels)\n",
    "dataset_train_labels.register(workspace=ws, name='mnist_train_labels', description='MNIST input dataset for train labels', create_new_version = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the ml.azure.com portal to check whether the datasets are registered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m \"added datasets and versioned them\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train ML Model in a Reproducible Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Traditional way of running a experiment\n",
    "Now run a simple Tensorflow job here to train an image classifier base off of MNIST dataset. This example requires Tensorflow 1.13.\n",
    "\n",
    "This is a simple two later fully connected neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import load_data\n",
    "\n",
    "data_folder = os.path.join('data', 'mnist')\n",
    "\n",
    "print('training dataset is stored here:', data_folder)\n",
    "\n",
    "training_set_size = X_train.shape[0]\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    # construct the DNN\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "    h1 = tf.layers.dense(X, n_h1, activation=tf.nn.relu, name='h1')\n",
    "    h2 = tf.layers.dense(h1, n_h2, activation=tf.nn.relu, name='h2')\n",
    "    output = tf.layers.dense(h2, n_outputs, name='output')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Make this job more reproducible - Still not ideal\n",
    "\n",
    "A better way to train this NN model is to track the metrics (the better approach will be discussed later today)\n",
    "\n",
    "**Tracking the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='Train-TF-Locally')\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Hyper Parameters and Accuracy metrics for each epoch. Finally I collect the accuracy for the final epoch as the final accuracy metric.\n",
    "\n",
    "Then I upload the model into the output section of the Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_row('Hyper Parameters',\n",
    "        n_inputs=n_inputs,\n",
    "        n_h1=n_h1,\n",
    "        n_h2=n_h2,\n",
    "        n_outputs=n_outputs,\n",
    "        learning_rate=learning_rate,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_images = Dataset.get_by_name(workspace = ws, name = 'mnist_test_images')\n",
    "dataset_test_labels = Dataset.get_by_name(workspace = ws, name = 'mnist_test_labels')\n",
    "dataset_train_images = Dataset.get_by_name(workspace = ws, name = 'mnist_train_images')\n",
    "dataset_train_labels = Dataset.get_by_name(workspace = ws, name = 'mnist_train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_row('dataset_test_images', dataset_name='mnist_test_images', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_test_labels', dataset_name='mnist_test_labels', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_train_images', dataset_name='mnist_train_images', version=dataset_train_labels.version)\n",
    "run.log_row('dataset_train_labels', dataset_name='mnist_train_labels', version=dataset_train_labels.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_images.download('data/from_dataset')\n",
    "dataset_test_labels.download('data/from_dataset')\n",
    "dataset_train_images.download('data/from_dataset')\n",
    "dataset_train_labels.download('data/from_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the latest version of registered Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_data('./data/from_dataset/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/from_dataset/train-labels.gz', True).reshape(-1)\n",
    "X_test = load_data('./data/from_dataset/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/from_dataset/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        run.log('accuracy-train', acc_train)\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "        run.log('accuracy-val', acc_val)\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    run.log('final-accuracy', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n",
    "    run.upload_folder('/outputs/model/', './outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all or one of the metrics:\n",
    "\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 A better way to submit the TF job\n",
    "\n",
    "Instead of running the TF job on our local laptop (or notebooks.azure.com) there is an easier way to accomplish this task. You can:\n",
    "1. Create a remote compute - this can be a single VM, a set of VM (Azure ML Compute), Databricks or HDInsight Cluster (Spark).\n",
    "1. Instead of we dealing with the depandancies, we let Azure ML manage the dependencies. In other words, We tell it what is the type of script, whether it's TF, Pytorch or just a python script and we let Azure set up the create remote compute instance for us.\n",
    "1. We mount the dataset we created above to the compute node and we let Azure stream the data from the storage to the node\n",
    "1. We ask Azure to register the model into a Model management instead of the output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infra as Code: Creating a remote computer\n",
    "\n",
    "With a couple of lines, we create a GPU cluster ([NV6 \"1 NVIDIA Tesla M60\"](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu#nv-series)) of 1 node. (We set it to 1 as we don't want to burn the credit - in real production settings we go with much larger VM size and much more than 1 node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPU cluster of type NV6 with 1 node. (due to subscription's limitations we stick to 1 node)\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpuclusterNV\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AmlCompute.provisioning_configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of compute targets\n",
    "\n",
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps from now:\n",
    "1. Create a train.py file which keeps contains the training code for the MNIST TF file\n",
    "    1. At the beginning of the train.py we define the arguments that we want to pass to the training file\n",
    "1. It has all of the other dependencies such as util.py\n",
    "1. We create a dictionary object that keeps all of the parameters need to be passed to the train.py\n",
    "1. We define an estimator object from azureml.train.dnn.TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the project-folder that keeps all of the files required to be passed to the remote node\n",
    "\n",
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"project-folder\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Providing the dataset as an input parameter\n",
    "\n",
    "One of the ways to make the training job flexible is to provide parameters instead of counting on hardcoded values. One of the important values is the data folder. As we've discussed the two ways of providing data to the ML Workspace (Datastores and Datasets) you can similarly provide both to your training job. In the example below, we have demostrated both method. As the Datasets requires us to install extra packages on the remote compute and this takes us longer to execute, we chose to use the Datastore for this example.\n",
    "\n",
    "The as_mount function connects the dataset or datastore to the target compute at the run time, makes it look like accessing the files as if an external storage is mounted to the remote compute. This is very efficient, as it doesn't bring un-accessed data into the remote compute.\n",
    "\n",
    "The other options is to call as_download function, but it requires the remote compute to fiest download the entire data within datastore or dataset before running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    # You can provide the path from the Data Store or\n",
    "    '--data-folder': ws.get_default_datastore().as_mount(),\n",
    "    # Provide the mount point to the latest version of the registered dataset. If you choose Dataset instead of Datastore,\n",
    "    # you need to install azureml-dataprep[fuse,pandas] on the compute node using pip_packages parameter \"pip_packages=['azureml-dataprep[fuse,pandas]']\"\n",
    "    # '--dataset-test-images': dataset_test_images.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-test-labels': dataset_test_labels.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-train-images': dataset_train_images.as_named_input('mnist').as_mount(),\n",
    "    # '--dataset-train-labels': dataset_train_labels.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 # pip_packages=['azureml-dataprep[fuse,pandas]'],\n",
    "                 # pip_packages = ['pandas', 'numpy', '...']\n",
    "                 # conda_packages = ['pandas', 'numpy', '...']\n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to **Tensorflow** class, you can submit Python script with **Pytorch**, **Chainer**, **SKLearn** and pure **PythonScript** classes to the remote **compute target**. Further reading: https://github.com/Azure/AzureML-Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Register the model into the Model Registry\n",
    "\n",
    "The Run object can access to the output folder that is saved under the current Run within the experiment. By using the register_model function, you can register the model under the Model tab of the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model(model_name='tf-dnn-mnist-single-run', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Environment Logging and Reproducibility\n",
    "\n",
    "When the script is submitted using an Estimator object, Azure ML deploys a GPU base Linux docker image on the remote compute and based on the pip_packages and conda_packages parameters you can reproduce the environment as you wish.\n",
    "\n",
    "Once the Run is submitted, the entire environment dependencies are logged and can be reproduced. Using the Run Details, you can explore the docker, conda and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly define the characteristics of the environment trhrough **RunConfiguration** class and provide it to the ScriptSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.runconfig import RunConfiguration\n",
    "# from azureml.core.conda_dependencies import CondaDependencies\n",
    "# from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "# \n",
    "# # create a new runconfig object\n",
    "# run_config = RunConfiguration()\n",
    "# \n",
    "# # enable Docker \n",
    "# run_config.environment.docker.enabled = True\n",
    "# \n",
    "# # set Docker base image to the default CPU-based image\n",
    "# run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# \n",
    "# # use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "# run_config.environment.python.user_managed_dependencies = False\n",
    "# \n",
    "# # specify CondaDependencies obj\n",
    "# run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "#                                                                                           'numpy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Hyperparameter tuning\n",
    "\n",
    "In most of our DS experiments, in order to get to the optimal model, you have to try several hyper parameters on your algorithm. This can become super slow and time consuming. Using HyperDrive namespace, you can provide several choices to be passed to your training script through the parameters. In the example below, I provided three choices for the batch-size, 5 choices for the first layer neurons, 4 choices for the second-layer-neurons and a random value from the continuous space of log uniform distribution for the learning-rate.\n",
    "\n",
    "Later by setting the max_total_runs as a parameter to the HyperDriveConfig, you can set the total number of tries. In the example below, it uses Random Sampling technique to find the next combination of hyperparameters to try. It will stop based on the BanditPolicy or if number of iterations reaches the max_total_runs.\n",
    "\n",
    "Here are the types of sampling the hyperparamer space:\n",
    "* Random sampling\n",
    "* Grid sampling\n",
    "* Bayesian sampling\n",
    "\n",
    "Link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters\n",
    "\n",
    "In case of having several nodes (in our case we only have 1 node), you can submit several parallel execution on multiple nodes by setting the max_concurrent_runs. For example, if you have set max_total_runs = 100 and set max_concurrent_runs to 20. Assuming that you have 20 nodes of GPU cluster, then there will be 20 concurrent runs on all of the 20 nodes. Therefore, in theory you require 5 cycles to compelete the entire 100 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n",
    "        '--second-layer-neurons': choice(10, 50, 200, 500),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params={'--data-folder': ws.get_default_datastore().as_mount()},\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveConfig(estimator=est, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='accuracy-val', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=4,\n",
    "                       max_concurrent_runs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this run as you'll be having several models to collect. You can always get the best or collect the entire generated models. In the example below, the best model is retrieved and registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()\n",
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist-hyperp-tunning', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you like to delete the compute target - If you like to continue the tutorial, \n",
    "# please keep the compute target on.\n",
    "\n",
    "# compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m \"Added the Hyperparameter section\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
