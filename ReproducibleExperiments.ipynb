{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reproducible Experiments\n",
    "In this notebook, the aim is to show you how to build a reproducible experiment within Azure. In this example, we're going to train a MNIST classifier using Tensorflow and show you how you can follow some practices to make this work reproducible. Later you can use this framework and apply it to other ML problems.\n",
    "\n",
    "The reason we chose MNIST is to pick a very simple example as the main focus is to build a reproducible experiment and not to learn a new algorithm or to build a complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:**:\n",
    "\n",
    "In order to practice all parts of the following Notebook, you first need to get a free Azure credit. If you don't have it, you can simply obtain it through this link: https://azure.microsoft.com/en-us/free/\n",
    "\n",
    "You can run this notebook on your local latop, Azure Notebooks (notebooks.azure.com) or Notebook VMs:\n",
    "- Local Laptop - the following packages has to be installed:\n",
    "    - Azureml-SDK - with notebook,widget extensions\n",
    "    - tensorflow==1.13\n",
    "- Azure Notebooks:\n",
    "    - This is a free notebook, all of the packages for an ML experiment is installed\n",
    "- AzureML Notebook:\n",
    "    - This is a premium notebook that you can choose the VM type. Avoid using this feature for the workshop as you may burn your credit before the end or the workshop.\n",
    "\n",
    "Once you chose the execution environment, you need to create an Azure Machine Learning Service. Follow this instruction to build one:\n",
    "\n",
    "The following text is copied from: https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup#create-a-workspace\n",
    "\n",
    "\n",
    "An Azure Machine Learning workspace is a foundational resource in the cloud that you use to experiment, train, and deploy machine learning models. It ties your Azure subscription and resource group to an easily consumed object in the service.\n",
    "\n",
    "You create a workspace via the Azure portal, a web-based console for managing your Azure resources.\n",
    "\n",
    "1. Sign in to the Azure portal by using the credentials for the Azure subscription you use.\n",
    "1. In the upper-left corner of Azure portal, select + Create a resource.\n",
    "1. Create a new resource\n",
    "1. Use the search bar to find Machine Learning service workspace.\n",
    "1. Select Machine Learning service workspace.\n",
    "1. In the Machine Learning service workspace pane, select Create to begin.\n",
    "1. Provide the following information to configure your new workspace:\n",
    "    - **Field\tDescription**\n",
    "    - **Workspace name**: type in **FirstExample**.\n",
    "    - **Subscription**: Select the Azure subscription that you want to use. (Your free credit)\n",
    "    - **Resource group**: type in **MLOpsWorkshop**\n",
    "    - **Location**: type in **westus2**\n",
    "1. After you are finished configuring the workspace, select Create.\n",
    "When the process is finished, a deployment success message appears.\n",
    "1. To view the new workspace, select Go to resource.\n",
    "\n",
    "\n",
    "You can explore the resource from two view:\n",
    "1. https://portal.azure.com (you can access all resources including Azure ML)\n",
    "1. https://ml.azure.com (recently released - still in preview and dedicated to Azure ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download the MNIST sample files from Yann Lecun website to our development environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/mnist/test-labels.gz', <http.client.HTTPMessage at 0x227a9c41308>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a bunch of packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the AzureML SDK package to be able to communicate with Azure ML Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.65\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your subscription ID will be different replace the stirng with yours\n",
    "subscription_id = \"<subscription_id>\"\n",
    "resource_group = \"MLOpsWorkshop\"\n",
    "workspace_name = \"FirstExample\"\n",
    "workspace_region = \"westus2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an object from Workspace class. the Workspace object will point to the created Workspace we created through the portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Workspace class and check the azureml SDK version\n",
    "# exist_ok checks if workspace exists or not.\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: FirstExample\n",
      "Azure region: westus2\n",
      "Subscription id: ed70929a-e125-4daf-8945-04f709d2c75e\n",
      "Resource group: MLOpsWorkshop\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a new Experiment\n",
    "\n",
    "In software engineering world we have a new feature to develop. In ML/Data Science world, we work on experiments.\n",
    "\n",
    "**Experiments** represent the collection of trials used to validate a user's hypothesis. We call each trial a run.\n",
    "\n",
    "Here we create a new experiment, we want to make sure everything related to this experiment is saved within the workspace and the lineage between each artifact is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tracking-logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Track and Log Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two ways to start a trail (Run). Interactively and through batch submite. The start_loggin() method is the interactive way of starting a trail. It returns a Run object that we can use to log important metrics or the trail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key metrics can a single value for the accuracy of an ML model, a list of values representing the distribution or the data or an image showing the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing log tracking action by creating a Run object in the Experiment\n",
    "run =  exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see that the **azureml.git.repository_uri** is poinint to the remote repo and the **azureml.git.branch** property is poining to the active branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': '19043492-f61a-4d4b-ae1b-044d4e478082',\n",
       " 'target': 'sdk',\n",
       " 'status': 'Running',\n",
       " 'startTimeUtc': '2019-10-16T20:46:13.46428Z',\n",
       " 'properties': {'azureml.git.repository_uri': 'https://github.com/classicboyir/exp-repro.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/classicboyir/exp-repro.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': 'ad924179738fbf1cdbc68752c5e0121c74df1c9b',\n",
       "  'mlflow.source.git.commit': 'ad924179738fbf1cdbc68752c5e0121c74df1c9b',\n",
       "  'azureml.git.dirty': 'True',\n",
       "  'ContentSnapshotId': 'c6e2b072-2458-4e31-af43-77c130631562'},\n",
       " 'inputDatasets': [],\n",
       " 'logFiles': {}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunDetails** class helps you visualize the active run object. It creates a network connection with Azure ML Worspace to collect everything happening during the run. It gets updated every 15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c813aeeaf3ff458ebb640119aef8125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logs** function will log a single-valued or multi-valued metric under the current RUN. There are several types of logs, metric, table, row, image, etc.\n",
    "\n",
    "Every time you add a new metric, check the widget above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('metric_1', 6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some metrics about the input dataset:\n",
    "    \n",
    "from utils import load_data\n",
    "\n",
    "# Unzipping the input dataset and conver the data points into Numpy arrays\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record how the input dataset looks like\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "    \n",
    "run.log_image(name='{}-samples-of-input-dataset'.format(sample_size), plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dist = pd.DataFrame(data=y_train, columns=['test_values'])['test_values'].value_counts()\n",
    "\n",
    "run.log_table('digit_dist', {\"count\":list(dist.values), \"digits\":list(dist.index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('train_dataset_size', X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('test_dataset_size', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Datasets\n",
    "\n",
    "As an important part of experiment reproducibility, you'd like to separate your dataset from the training code and the development environment. There are two major ways that you can achieve this. First you can use datastores to define the connection to an Azure data store such as Blob, SQL or Databricks table, then you leverage Datasets to access the actual files and version the reference to those file assets.\n",
    "\n",
    "### 2.1 Generate data references for the ML job\n",
    "\n",
    "Once an ML Workspace is created, a storage account is created with a default container (a logical container that works as a folder - this is refered to as bucket in AWS S3). The container is attached to the Workspace automatically as the default storage account. You can find it as **workspaceblobstore** under Datastores (https://ml.azure.com). This storage account can be used for test and development but should not be used in production scenarios. Because if you decide to delete the Workspace, the default storage account is also deleted which results in losing your data. So it's wiser to create a separate storage account and attach it to the ML Workspace.\n",
    "\n",
    "Here is how to access the default storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'workspaceblobstore'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_default_datastore returns the default datastore attached to the Workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name of the default datastore suggestion, it's a reference to a Blob storage. Blob storage is a general purpose data lake that can be used to store any type of binary, from image, to csv file. Here are other types of Azure Storage types that you can attach to the WorkSpace:\n",
    "\n",
    "- Azure File Share\n",
    "- Azure Data Lake\n",
    "- Azure Data Lake Gen2\n",
    "- Azure SQL Database\n",
    "- Azure PostgreSQL\n",
    "- Databricks File System\n",
    "\n",
    "The following is an example of registering (attaching) a new Blob storage account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.datastore import Datastore\n",
    "# blob_datastore = Datastore.register_azure_blob_container(\n",
    "#            workspace=ws,\n",
    "#            datastore_name='<datastore_name>',\n",
    "#            account_name='<account_name>', # Storage account name\n",
    "#            container_name='<container_name>', # Name of Azure blob container\n",
    "#            account_key='<account_key>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the files that I downloaded to this storage for longer retention. This datastore can be referenced later in my training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 4 files\n",
      "Uploading ./data/mnist\\test-images.gz\n",
      "Uploading ./data/mnist\\test-labels.gz\n",
      "Uploading ./data/mnist\\train-images.gz\n",
      "Uploading ./data/mnist\\train-labels.gz\n",
      "Uploaded ./data/mnist\\test-labels.gz, 1 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\train-labels.gz, 2 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\test-images.gz, 3 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\train-images.gz, 4 files out of an estimated total of 4\n",
      "Uploaded 4 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_036fd2f51eb84b0f9b07a1604563e049"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate Datasets\n",
    "\n",
    "There are two different types of Datasets, TabularDataset and FileDataset. The Tabular can be used to access tabular like datasources, such as csv, SQL, Databricks, etc which FileDatasets can be used for binary datasets such as image, audio, etc. The following is a way to define a tabular dataset from an online source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500              S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250              S  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# create a TabularDataset from a delimited file behind a public web url\n",
    "web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path)\n",
    "\n",
    "# To convert the Dataset into Spark (you need to have Spark installed on your development environment)\n",
    "# titanic_ds.take(3).to_spark_dataframe()\n",
    "\n",
    "# preview the first 3 rows of titanic_ds\n",
    "titanic_ds.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are references to a datasource (registered under datasources or available over internet). In other words, they don't keep the data, they are the reference definitiosn. You can register the defined Datasets under the Datasets section (accessible through ml.azure.com). Each time you register the dataset under the same name, you'll get a new version generated. Later you can access a particular version for the registered Dataset. As the dataset doesn't store your data, if you remove or change the data, the Dataset object doesn't help you roll back the change. Therefore, it's recommended to keep the data untouched. \n",
    "\n",
    "In order to keep the actual dataset, you can copy the data (using Azure Data Factory SDK or AzCopy CLI tool or [Azure SDK](https://github.com/Azure/azure-sdk-for-python/tree/master/sdk/storage/azure-storage-blob)) to clone the data to a new blob location and keep the under a new Dataset object.\n",
    "\n",
    "In the following cells, I've generated two versions of the Dataset. Every version of the titanic_ds can have different reference structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ds = titanic_ds.register(workspace = ws,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'titanic training data',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time you can retrive a particular version of Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500              S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250              S  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_ds_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'titanic_ds', \n",
    "                                 version = 1)\n",
    "\n",
    "titanic_ds_v1.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample code to access Parquet files: link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets\n",
    "#     \n",
    "# # create a TabularDataset with time series trait\n",
    "# datastore_paths = [(datastore, 'weather/*/*/*/data.parquet')]\n",
    "# \n",
    "# # get a coarse timestamp column from the path pattern\n",
    "# dataset = Dataset.Tabular.from_parquet_files(path=datastore_path, partition_format='weather/{coarse_time:yyy/MM/dd}/data.parquet')\n",
    "# \n",
    "# # set coarse timestamp to the virtual column created, and fine grain timestamp from a column in the data\n",
    "# dataset = dataset.with_timestamp_columns(fine_grain_timestamp='datetime', coarse_grain_timestamp='coarse_time')\n",
    "# \n",
    "# # filter with time-series-trait-specific methods\n",
    "# data_slice = dataset.time_before(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_after(datetime(2019, 1, 1))\n",
    "# data_slice = dataset.time_between(datetime(2019, 1, 1), datetime(2019, 2, 1))\n",
    "# data_slice = dataset.time_recent(timedelta(weeks=1, days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going back to our MNIST problem, let's create a Dataset object from the file uploaded to the default Datastore and register the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'mnist/test-images.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/test-labels.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/train-images.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/train-labels.gz')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore_paths = [(ds, 'mnist/test-images.gz'),\n",
    " (ds, 'mnist/test-labels.gz'),\n",
    " (ds, 'mnist/train-images.gz'),\n",
    " (ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "mnist_dataset = Dataset.File.from_files(datastore_paths)\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's register it within the Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'mnist/test-images.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/test-labels.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/train-images.gz')\",\n",
       "    \"('workspaceblobstore', 'mnist/train-labels.gz')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"name\": \"mnist_dataset\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"MNIST input dataset\",\n",
       "    \"workspace\": \"Workspace.create(name='FirstExample', subscription_id='ed70929a-e125-4daf-8945-04f709d2c75e', resource_group='MLOpsWorkshop')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define a new Dataset under **mnist_dataset** by accessing the actual file over the internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"http://lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
       "    \"http://lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
       "    \"http://lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
       "    \"http://lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"name\": \"mnist_dataset\",\n",
       "    \"version\": 2,\n",
       "    \"description\": \"MNIST input dataset\",\n",
       "    \"workspace\": \"Workspace.create(name='FirstExample', subscription_id='ed70929a-e125-4daf-8945-04f709d2c75e', resource_group='MLOpsWorkshop')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_paths = [\n",
    "            'http://lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "\n",
    "mnist_dataset_web = Dataset.File.from_files(path = web_paths)\n",
    "\n",
    "mnist_dataset_web.register(workspace = ws,\n",
    "                                 name = 'mnist_dataset',\n",
    "                                 description = 'MNIST input dataset',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_v1 = Dataset.get_by_name(workspace = ws,\n",
    "                                 name = 'mnist_dataset', \n",
    "                                 version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/workspaceblobstore/mnist/test-images.gz',\n",
       "       '/workspaceblobstore/mnist/test-labels.gz',\n",
       "       '/workspaceblobstore/mnist/train-images.gz',\n",
       "       '/workspaceblobstore/mnist/train-labels.gz'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of registered files back\n",
    "mnist_dataset_v1.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the level of flexibility, I create a different dataset for each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'mnist/train-labels.gz')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"name\": \"mnist_train_labels\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"MNIST input dataset for train labels\",\n",
       "    \"workspace\": \"Workspace.create(name='FirstExample', subscription_id='ed70929a-e125-4daf-8945-04f709d2c75e', resource_group='MLOpsWorkshop')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore_path_test_images = [(ds, 'mnist/test-images.gz')]\n",
    "datastore_path_test_labels = [(ds, 'mnist/test-labels.gz')]\n",
    "datastore_path_train_images = [(ds, 'mnist/train-images.gz')]\n",
    "datastore_path_train_labels = [(ds, 'mnist/train-labels.gz')]\n",
    "\n",
    "dataset_test_images = Dataset.File.from_files(datastore_path_test_images)\n",
    "dataset_test_images.register(workspace=ws, name='mnist_test_images', description='MNIST input dataset for test images', create_new_version = True)\n",
    "\n",
    "dataset_test_labels = Dataset.File.from_files(datastore_path_test_labels)\n",
    "dataset_test_labels.register(workspace=ws, name='mnist_test_labels', description='MNIST input dataset for test labels', create_new_version = True)\n",
    "\n",
    "dataset_train_images = Dataset.File.from_files(datastore_path_train_images)\n",
    "dataset_train_images.register(workspace=ws, name='mnist_train_images', description='MNIST input dataset for train images', create_new_version = True)\n",
    "\n",
    "dataset_train_labels = Dataset.File.from_files(datastore_path_train_labels)\n",
    "dataset_train_labels.register(workspace=ws, name='mnist_train_labels', description='MNIST input dataset for train labels', create_new_version = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.File.from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c68a6c8] added datasets and versioned them\n",
      " 3 files changed, 722 insertions(+), 92 deletions(-)\n",
      " create mode 100644 .gitignore\n",
      " create mode 100644 utils.py\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"added datasets and versioned them\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Dataset\n",
    "\n",
    "As part of building a reproducible experiment, we need to separate the input data from the code and also from the compute environment.\n",
    "\n",
    "There are multiple ways to register a dataset within Azure ML. \n",
    "\n",
    "* Store it on the Blob storage and create a reference to the Experiment\n",
    "\n",
    "* Register the source data as Dataset artifact and register it at the Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional way of running a experiment\n",
    "Now run a simple Tensorflow job here to train an image classifier base off of MNIST dataset. This example requires Tensorflow 1 (1.14 in this case)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - From <ipython-input-42-2b90fbe99597>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING - From c:\\users\\hosarsha\\appdata\\local\\conda\\conda\\envs\\azureml\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset is stored here: data\\mnist\n",
      "0 -- Training accuracy: 0.8 Validation accuracy: 0.9\n",
      "1 -- Training accuracy: 0.96 Validation accuracy: 0.9143\n",
      "2 -- Training accuracy: 0.98 Validation accuracy: 0.9256\n",
      "3 -- Training accuracy: 0.98 Validation accuracy: 0.9336\n",
      "4 -- Training accuracy: 0.92 Validation accuracy: 0.938\n",
      "5 -- Training accuracy: 0.94 Validation accuracy: 0.9426\n",
      "6 -- Training accuracy: 0.96 Validation accuracy: 0.9455\n",
      "7 -- Training accuracy: 0.94 Validation accuracy: 0.9488\n",
      "8 -- Training accuracy: 0.98 Validation accuracy: 0.9515\n",
      "9 -- Training accuracy: 0.96 Validation accuracy: 0.956\n",
      "10 -- Training accuracy: 0.98 Validation accuracy: 0.9579\n",
      "11 -- Training accuracy: 0.92 Validation accuracy: 0.9597\n",
      "12 -- Training accuracy: 0.96 Validation accuracy: 0.9602\n",
      "13 -- Training accuracy: 1.0 Validation accuracy: 0.9617\n",
      "14 -- Training accuracy: 0.92 Validation accuracy: 0.9632\n",
      "15 -- Training accuracy: 0.96 Validation accuracy: 0.9642\n",
      "16 -- Training accuracy: 0.96 Validation accuracy: 0.9664\n",
      "17 -- Training accuracy: 0.98 Validation accuracy: 0.9655\n",
      "18 -- Training accuracy: 0.96 Validation accuracy: 0.968\n",
      "19 -- Training accuracy: 1.0 Validation accuracy: 0.9669\n",
      "Final accuracy (val):  0.9669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import load_data\n",
    "\n",
    "data_folder = os.path.join('data', 'mnist')\n",
    "\n",
    "print('training dataset is stored here:', data_folder)\n",
    "\n",
    "training_set_size = X_train.shape[0]\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_h1 = 100\n",
    "n_h2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    # construct the DNN\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "    h1 = tf.layers.dense(X, n_h1, activation=tf.nn.relu, name='h1')\n",
    "    h2 = tf.layers.dense(h1, n_h2, activation=tf.nn.relu, name='h2')\n",
    "    output = tf.layers.dense(h2, n_outputs, name='output')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make this job more reproducible - Still the Manual Way\n",
    "\n",
    "A more manual way of tracking metrics - (the better approach will be discussed later today)\n",
    "\n",
    "**Tracking the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Hyper Parameters\n",
    "\n",
    "run.log_row('Hyper Parameters',\n",
    "        n_inputs=n_inputs,\n",
    "        n_h1=n_h1,\n",
    "        n_h2=n_h2,\n",
    "        n_outputs=n_outputs,\n",
    "        learning_rate=learning_rate,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- Training accuracy: 0.8 Validation accuracy: 0.8968\n",
      "1 -- Training accuracy: 0.92 Validation accuracy: 0.9166\n",
      "2 -- Training accuracy: 0.86 Validation accuracy: 0.9266\n",
      "3 -- Training accuracy: 0.96 Validation accuracy: 0.9314\n",
      "4 -- Training accuracy: 0.98 Validation accuracy: 0.9393\n",
      "5 -- Training accuracy: 0.94 Validation accuracy: 0.9442\n",
      "6 -- Training accuracy: 0.94 Validation accuracy: 0.9488\n",
      "7 -- Training accuracy: 0.96 Validation accuracy: 0.9526\n",
      "8 -- Training accuracy: 0.94 Validation accuracy: 0.9552\n",
      "9 -- Training accuracy: 0.96 Validation accuracy: 0.9572\n",
      "10 -- Training accuracy: 0.94 Validation accuracy: 0.9589\n",
      "11 -- Training accuracy: 0.96 Validation accuracy: 0.962\n",
      "12 -- Training accuracy: 0.96 Validation accuracy: 0.9636\n",
      "13 -- Training accuracy: 0.94 Validation accuracy: 0.9646\n",
      "14 -- Training accuracy: 0.98 Validation accuracy: 0.9663\n",
      "15 -- Training accuracy: 1.0 Validation accuracy: 0.9669\n",
      "16 -- Training accuracy: 0.98 Validation accuracy: 0.9679\n",
      "17 -- Training accuracy: 0.94 Validation accuracy: 0.9686\n",
      "18 -- Training accuracy: 0.98 Validation accuracy: 0.9692\n",
      "19 -- Training accuracy: 0.98 Validation accuracy: 0.9696\n",
      "Final accuracy (val):  0.9696\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # randomly shuffle training set\n",
    "        indices = np.random.permutation(training_set_size)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        # batch index\n",
    "        b_start = 0\n",
    "        b_end = b_start + batch_size\n",
    "        for _ in range(training_set_size // batch_size):\n",
    "            # get a batch\n",
    "            X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "            # update batch index for the next batch\n",
    "            b_start = b_start + batch_size\n",
    "            b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "            # train\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        # evaluate training set\n",
    "        acc_train = acc_op.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        run.log('accuracy-train', acc_train)\n",
    "        # evaluate validation set\n",
    "        acc_val = acc_op.eval(feed_dict={X: X_test, y: y_test})\n",
    "        run.log('accuracy-val', acc_val)\n",
    "\n",
    "        print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "        y_hat = np.argmax(output.eval(feed_dict={X: X_test}), axis=1)\n",
    "    \n",
    "    print('Final accuracy (val): ', acc_val)\n",
    "    run.log('final-accuracy', acc_val)\n",
    "    os.makedirs('./outputs/model', exist_ok=True)\n",
    "    # files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "    saver.save(sess, './outputs/model/mnist-tf.model')\n",
    "    run.upload_folder('/outputs/model/', './outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35634e5ba74b4c9e9fd479b2dde1c2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_1 {'metric_1': [1.1, 2, 2, 4, 4, 2.5, 6.3]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric_1': [1.1, 2, 2, 4, 4, 2.5, 6.3],\n",
       " '30-samples-of-input-dataset': 'aml://artifactId/ExperimentRun/dcid.3e943ed5-804b-41fc-afab-e351285371b5/30-samples-of-input-dataset_1570721355.png',\n",
       " 'digit_dist': {'count': [6742,\n",
       "   6265,\n",
       "   6131,\n",
       "   5958,\n",
       "   5949,\n",
       "   5923,\n",
       "   5918,\n",
       "   5851,\n",
       "   5842,\n",
       "   5421],\n",
       "  'digits': [1, 7, 3, 2, 9, 0, 6, 8, 4, 5]},\n",
       " 'train_dataset_size': 60000,\n",
       " 'test_dataset_size': 10000,\n",
       " 'Hyper Parameters': {'n_inputs': 784,\n",
       "  'n_h1': 100,\n",
       "  'n_h2': 100,\n",
       "  'n_outputs': 10,\n",
       "  'learning_rate': 0.01,\n",
       "  'n_epochs': 20,\n",
       "  'batch_size': 50},\n",
       " 'accuracy-train': [0.800000011920929,\n",
       "  0.9200000166893005,\n",
       "  0.8600000143051147,\n",
       "  0.9599999785423279,\n",
       "  0.9800000190734863,\n",
       "  0.9399999976158142,\n",
       "  0.9399999976158142,\n",
       "  0.9599999785423279,\n",
       "  0.9399999976158142,\n",
       "  0.9599999785423279,\n",
       "  0.9399999976158142,\n",
       "  0.9599999785423279,\n",
       "  0.9599999785423279,\n",
       "  0.9399999976158142,\n",
       "  0.9800000190734863,\n",
       "  1,\n",
       "  0.9800000190734863,\n",
       "  0.9399999976158142,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863],\n",
       " 'accuracy-val': [0.8967999815940857,\n",
       "  0.9165999889373779,\n",
       "  0.9265999794006348,\n",
       "  0.9314000010490417,\n",
       "  0.939300000667572,\n",
       "  0.9441999793052673,\n",
       "  0.9488000273704529,\n",
       "  0.9526000022888184,\n",
       "  0.9552000164985657,\n",
       "  0.9571999907493591,\n",
       "  0.958899974822998,\n",
       "  0.9620000123977661,\n",
       "  0.9635999798774719,\n",
       "  0.9646000266075134,\n",
       "  0.9663000106811523,\n",
       "  0.9668999910354614,\n",
       "  0.9678999781608582,\n",
       "  0.9685999751091003,\n",
       "  0.9692000150680542,\n",
       "  0.9696000218391418],\n",
       " 'final-accuracy': 0.9696000218391418}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all or one of the metrics:\n",
    "\n",
    "print('metric_1: ', run.get_metrics('metric_1'))\n",
    "\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "ScriptRunConfig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\mnist'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'..\\\\' + data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the TF job to a Remote Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a remote computer\n",
    "\n",
    "This is to make a remote instance to run our TF job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-10-10T16:12:06.692000+00:00', 'errors': None, 'creationTime': '2019-10-10T16:10:47.133007+00:00', 'modifiedTime': '2019-10-10T16:11:03.644102+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': ''}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NV6'}\n"
     ]
    }
   ],
   "source": [
    "# Create a GPU cluster of type NV6 with 1 node. (due to subscription's limitations we stick to 1 node)\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpucluster AmlCompute Succeeded\n"
     ]
    }
   ],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the input data at a cloud storage (Blob storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\CloneOfOneDrive\\Private\\Talks\\MLOps\\Workshop Materials\\Day1\\ExperimentRepo\\project-folder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"project-folder\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 4 files\n",
      "Uploading ./data/mnist\\test-images.gz\n",
      "Uploading ./data/mnist\\test-labels.gz\n",
      "Uploading ./data/mnist\\train-images.gz\n",
      "Uploading ./data/mnist\\train-labels.gz\n",
      "Uploaded ./data/mnist\\test-labels.gz, 1 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\train-labels.gz, 2 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\test-images.gz, 3 files out of an estimated total of 4\n",
      "Uploaded ./data/mnist\\train-images.gz, 4 files out of an estimated total of 4\n",
      "Uploaded 4 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_6a15e3cfdd5c4656aa653c4ac473bfde"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ws.get_default_datastore().as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=False, \n",
    "                 framework_version='1.13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>tracking-logs</td><td>tracking-logs_1570725323_7add8b27</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/ed70929a-e125-4daf-8945-04f709d2c75e/resourceGroups/MLOpsWorkshop/providers/Microsoft.MachineLearningServices/workspaces/FirstExample/experiments/tracking-logs/runs/tracking-logs_1570725323_7add8b27\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: tracking-logs,\n",
       "Id: tracking-logs_1570725323_7add8b27,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c705f912f0064cbaac2796b6d3dedb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "web_paths = [\n",
    "            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.file_dataset.FileDataset"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Dataset.File.from_files(path = web_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.register(workspace = ws,\n",
    "                           name = 'mnist_dataset',\n",
    "                           description='training and test dataset',\n",
    "                           create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(workspace = ws, name='mnist_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hosarsha\\appdata\\local\\conda\\conda\\envs\\azureml\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py:681: UserWarning: Please install pyarrow>=0.11.0 for improved performance of to_pandas_dataframe. You can ensure the correct version is installed by running: pip install azureml-dataprep[pandas].\n",
      "  warnings.warn('Please install pyarrow>=0.11.0 for improved performance of to_pandas_dataframe. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['/http/yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
       "       '/http/yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
       "       '/http/yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
       "       '/http/yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If environment_definition or conda_dependencies_file_path is specified, Azure ML will not install any framework related packages on behalf of the user.\n",
      "WARNING - This compute target type doesn't support non-Docker runs; overriding run configuration enable Docker.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['tensorflow']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': dataset.as_named_input('mnist_dataset').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 100,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('my_env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk','azureml-dataprep[pandas,fuse]>=1.1.14', 'tensorflow==1.14'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist_dataset.py', \n",
    "                 environment_definition=env,\n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(25, 50, 100),\n",
    "        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n",
    "        '--second-layer-neurons': choice(10, 50, 200, 500),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params={'--data-folder': ws.get_default_datastore().as_mount()},\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveConfig(estimator=est, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='accuracy-val', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=4,\n",
    "                       max_concurrent_runs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d5d7488e6f4a2caf5bc274e5deed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.train.restclients.hyperdrive.models.cancel_experiment_respose_dto.CancelExperimentResposeDto at 0x1ddbfc6dd08>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# htr.cancel()\n",
    "# compute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Once the training job is finished, you need to register the trained model in the Workspace's model management section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_azureml-execution-tvmps_82acb6821c4c777fb0f14707b02eacd1d5520931f054c96bdfad45c6bbc4e32d_d.txt', 'azureml-logs/65_job_prep-tvmps_82acb6821c4c777fb0f14707b02eacd1d5520931f054c96bdfad45c6bbc4e32d_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_82acb6821c4c777fb0f14707b02eacd1d5520931f054c96bdfad45c6bbc4e32d_d.txt', 'logs/azureml/141_azureml.log', 'logs/azureml/azureml.log', 'outputs/model/checkpoint', 'outputs/model/mnist-tf.model.data-00000-of-00001', 'outputs/model/mnist-tf.model.index', 'outputs/model/mnist-tf.model.meta']\n"
     ]
    }
   ],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()\n",
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist-project', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No go to the Workspace in the Azure Portal. Under the Model tab, you'll be able to see a model named **tf-dnn-mnist-project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Dependencies\n",
    "\n",
    "To provide other dependencies such as extra packages that need to be installed you can should provide the \"environment_definition\" to the Tensorflow class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpucluster AmlCompute Succeeded\n"
     ]
    }
   ],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tf-mnist\\\\utils.py'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 144K\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 17:07 .\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  6 22:27 ..\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  4 17:25 .azureml\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 14:38 data\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 17:06 .git\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 17:03 .ipynb_checkpoints\n",
      "-rwxrwxrwx 1 hossein hossein 113K Oct  7 17:07 mnist_base_example.ipynb\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  4 17:31 outputs\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 14:40 __pycache__\n",
      "drwxrwxrwx 1 hossein hossein 4.0K Oct  7 14:40 tf-mnist\n",
      "-rwxrwxrwx 1 hossein hossein 4.0K Oct  7 16:17 tf_mnist.py\n",
      "-rwxrwxrwx 1 hossein hossein 9.3K Oct  7 16:20 tf_mnist_with_summary.py\n",
      "-rwxrwxrwx 1 hossein hossein 6.1K Oct  7 14:16 Untitled.ipynb\n",
      "-rwxrwxrwx 1 hossein hossein  912 Oct  7 14:19 utils.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ws.get_default_datastore().as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py', \n",
    "                 use_gpu=False, \n",
    "                 framework_version='1.13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55725e4a0e44530956aaa063d7ef036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf-mnist_1570482481_e5a0a94d\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/ed70929a-e125-4daf-8945-04f709d2c75e/resourceGroups/MLOpsWorkshop/providers/Microsoft.MachineLearningServices/workspaces/FirstExample/experiments/tf-mnist/runs/tf-mnist_1570482481_e5a0a94d\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 141\n",
      "Entering Run History Context Manager.\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "TensorFlow version: 1.13.1\n",
      "training dataset is stored here: /mnt/batch/tasks/shared/LS_root/jobs/firstexample/azureml/tf-mnist_1570482481_e5a0a94d/mounts/workspaceblobstore/mnist\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "WARNING:tensorflow:From tf_mnist.py:51: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-10-07 21:08:32.117618: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-10-07 21:08:32.122896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2019-10-07 21:08:32.123797: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fc469acdf0 executing computations on platform Host. Devices:\n",
      "2019-10-07 21:08:32.123824: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "0 -- Training accuracy: 0.96 Validation accuracy: 0.9057\n",
      "1 -- Training accuracy: 0.96 Validation accuracy: 0.9247\n",
      "2 -- Training accuracy: 0.96 Validation accuracy: 0.9335\n",
      "3 -- Training accuracy: 0.92 Validation accuracy: 0.9382\n",
      "4 -- Training accuracy: 0.96 Validation accuracy: 0.9453\n",
      "5 -- Training accuracy: 1.0 Validation accuracy: 0.9492\n",
      "6 -- Training accuracy: 0.9 Validation accuracy: 0.9524\n",
      "7 -- Training accuracy: 1.0 Validation accuracy: 0.9551\n",
      "8 -- Training accuracy: 0.98 Validation accuracy: 0.9571\n",
      "9 -- Training accuracy: 0.98 Validation accuracy: 0.9601\n",
      "10 -- Training accuracy: 1.0 Validation accuracy: 0.9622\n",
      "11 -- Training accuracy: 0.98 Validation accuracy: 0.9619\n",
      "12 -- Training accuracy: 1.0 Validation accuracy: 0.9649\n",
      "13 -- Training accuracy: 0.98 Validation accuracy: 0.9658\n",
      "14 -- Training accuracy: 0.98 Validation accuracy: 0.9676\n",
      "15 -- Training accuracy: 1.0 Validation accuracy: 0.9675\n",
      "16 -- Training accuracy: 1.0 Validation accuracy: 0.9691\n",
      "17 -- Training accuracy: 0.98 Validation accuracy: 0.9696\n",
      "18 -- Training accuracy: 1.0 Validation accuracy: 0.9709\n",
      "19 -- Training accuracy: 0.98 Validation accuracy: 0.9704\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.0032188892364501953 seconds\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Starting job release. Current time:2019-10-07T21:09:13.989363\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 292\n",
      "Job release is complete. Current time:2019-10-07T21:09:18.384073\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf-mnist_1570482481_e5a0a94d\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/ed70929a-e125-4daf-8945-04f709d2c75e/resourceGroups/MLOpsWorkshop/providers/Microsoft.MachineLearningServices/workspaces/FirstExample/experiments/tf-mnist/runs/tf-mnist_1570482481_e5a0a94d\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1570482481_e5a0a94d',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-10-07T21:08:15.557214Z',\n",
       " 'endTimeUtc': '2019-10-07T21:09:28.365261Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'batchai',\n",
       "  'ContentSnapshotId': '8e2edf9b-3a2c-4e8c-91ee-0d7d10cc8472',\n",
       "  'azureml.git.repository_uri': 'https://github.com/classicboyir/reproducible-experiments.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/classicboyir/reproducible-experiments.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '1ff5103508538c170626fcc74004c8c934d383d4',\n",
       "  'mlflow.source.git.commit': '1ff5103508538c170626fcc74004c8c934d383d4',\n",
       "  'azureml.git.dirty': 'False',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'arguments': ['--data-folder',\n",
       "   '$AZUREML_DATAREFERENCE_workspaceblobstore',\n",
       "   '--batch-size',\n",
       "   '50',\n",
       "   '--first-layer-neurons',\n",
       "   '300',\n",
       "   '--second-layer-neurons',\n",
       "   '100',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster',\n",
       "  'dataReferences': {'workspaceblobstore': {'dataStoreName': 'workspaceblobstore',\n",
       "    'mode': 'Mount',\n",
       "    'pathOnDataStore': None,\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'data': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2019-10-07T18:41:03Z_212a5cff',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': True,\n",
       "    'condaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}],\n",
       "     'channels': ['conda-forge']},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'tensorflow:1.13-cpu',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': 'viennaprivate.azurecr.io',\n",
       "     'username': None,\n",
       "     'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/55_azureml-execution-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=G%2F44TiLmGt7x9ygHpnNeC6zsS5bwHaYpHFQT1omD3YU%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/65_job_prep-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=S2VSZFuNmWqIc4nfBcO5J0SXYcy%2BSGe4BMlHr%2F53UP0%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=ioKczmp5zCNuW3Dob4DXrRSFzOWDl2vCRNLetyuvZZw%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/75_job_post-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=VuAM3t0X7ae%2BFaTe4Bee155UNJhjQHdzlhME4HeCWSw%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r',\n",
       "  'logs/azureml/141_azureml.log': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/logs/azureml/141_azureml.log?sv=2018-11-09&sr=b&sig=%2FX7qGNxGcaXpKVi334GsiQgNdV9qs%2BQngwJ%2BPy5mJwg%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r',\n",
       "  'logs/azureml/azureml.log': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=hbJ%2FkMp%2FLih8Zzwm38BmYLZFf%2BSVB3crxY4vhdoncXw%3D&st=2019-10-07T20%3A59%3A32Z&se=2019-10-08T05%3A09%3A32Z&sp=r'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1570482481_e5a0a94d',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-10-07T21:08:15.557214Z',\n",
       " 'endTimeUtc': '2019-10-07T21:09:28.365261Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'batchai',\n",
       "  'ContentSnapshotId': '8e2edf9b-3a2c-4e8c-91ee-0d7d10cc8472',\n",
       "  'azureml.git.repository_uri': 'https://github.com/classicboyir/reproducible-experiments.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/classicboyir/reproducible-experiments.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '1ff5103508538c170626fcc74004c8c934d383d4',\n",
       "  'mlflow.source.git.commit': '1ff5103508538c170626fcc74004c8c934d383d4',\n",
       "  'azureml.git.dirty': 'False',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'arguments': ['--data-folder',\n",
       "   '$AZUREML_DATAREFERENCE_workspaceblobstore',\n",
       "   '--batch-size',\n",
       "   '50',\n",
       "   '--first-layer-neurons',\n",
       "   '300',\n",
       "   '--second-layer-neurons',\n",
       "   '100',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster',\n",
       "  'dataReferences': {'workspaceblobstore': {'dataStoreName': 'workspaceblobstore',\n",
       "    'mode': 'Mount',\n",
       "    'pathOnDataStore': None,\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'data': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2019-10-07T18:41:03Z_212a5cff',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': True,\n",
       "    'condaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}],\n",
       "     'channels': ['conda-forge']},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'tensorflow:1.13-cpu',\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': 'viennaprivate.azurecr.io',\n",
       "     'username': None,\n",
       "     'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/55_azureml-execution-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=Kr9eGQNEh9EeIBQ7Sl4KxjWteXQQw7cxbSOji5UuH28%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/65_job_prep-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=BZ5%2FMbVLN%2BEPsEWhYpvMGAEfpGlJsdOY3iGmf31ofFM%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=Y4dGUpcPIEagITNQpREsJSeKG7Z0oSaVQvNreRNAuAg%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/azureml-logs/75_job_post-tvmps_0a780cd4f5fcde0386523a2aa5c72fda2ee0cd3fa28292ae58ab0ab6f96de9b8_d.txt?sv=2018-11-09&sr=b&sig=7Y0AhD3%2FQMJ5izgmm%2BwOQkpTKWAsedlMIIYJXr1UqVE%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r',\n",
       "  'logs/azureml/141_azureml.log': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/logs/azureml/141_azureml.log?sv=2018-11-09&sr=b&sig=aAnJ%2FDurPLCf%2F%2BBXRNJ%2BN%2B7USh3cOzh9u4eDzhSwjl0%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r',\n",
       "  'logs/azureml/azureml.log': 'https://firstexample7340056478.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1570482481_e5a0a94d/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=6FAeTUC8TxxP4J5GLIlPp3DQcnDdFkP3Laa2mO69308%3D&st=2019-10-07T20%3A59%3A33Z&se=2019-10-08T05%3A09%3A33Z&sp=r'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_azureml-execution-tvmps_a8993b87913b0e2aee54a277a7a7f82a62a6b3427c6563e81acdadca064d1fa9_d.txt', 'azureml-logs/65_job_prep-tvmps_a8993b87913b0e2aee54a277a7a7f82a62a6b3427c6563e81acdadca064d1fa9_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_a8993b87913b0e2aee54a277a7a7f82a62a6b3427c6563e81acdadca064d1fa9_d.txt', 'logs/azureml/141_azureml.log', 'logs/azureml/azureml.log', 'outputs/model/checkpoint', 'outputs/model/mnist-tf.model.data-00000-of-00001', 'outputs/model/mnist-tf.model.index', 'outputs/model/mnist-tf.model.meta']\n"
     ]
    }
   ],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
